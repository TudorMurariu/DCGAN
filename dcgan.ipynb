{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c378ec",
   "metadata": {},
   "source": [
    "# Implementing the DCGAN in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6538c68",
   "metadata": {},
   "source": [
    "The Generator is trying to generate images mimicking a given input dataset. \n",
    "While the Discriminator's job is to judge if the image you generated can be grouped with the input dataset or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce84b99",
   "metadata": {},
   "source": [
    "##### Let's create a Generator classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5a52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LeakyReLU\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Sigmoid\n",
    "from torch import flatten\n",
    "from torch import nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, inputDim=100, outputChannels=1):\n",
    "        super(Generator, self).__init__()\n",
    "        # first set of CONVT => RELU => BN\n",
    "        self.ct1 = ConvTranspose2d(in_channels=inputDim,\n",
    "            out_channels=128, kernel_size=4, stride=2, padding=0,\n",
    "            bias=False)\n",
    "        self.relu1 = ReLU()\n",
    "        self.batchNorm1 = BatchNorm2d(128)\n",
    "        # second set of CONVT => RELU => BN\n",
    "        self.ct2 = ConvTranspose2d(in_channels=128, out_channels=64,\n",
    "                    kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.relu2 = ReLU()\n",
    "        self.batchNorm2 = BatchNorm2d(64)\n",
    "        # last set of CONVT => RELU => BN\n",
    "        self.ct3 = ConvTranspose2d(in_channels=64, out_channels=32,\n",
    "                    kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.relu3 = ReLU()\n",
    "        self.batchNorm3 = BatchNorm2d(32)\n",
    "        # apply another upsample and transposed convolution, but\n",
    "        # this time output the TANH activation\n",
    "        self.ct4 = ConvTranspose2d(in_channels=32,\n",
    "            out_channels=outputChannels, kernel_size=4, stride=2,\n",
    "            padding=1, bias=False)\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONVT => RELU => BN\n",
    "        # layers\n",
    "        x = self.ct1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batchNorm1(x)\n",
    "        # pass the output from previous layer through our second\n",
    "        # CONVT => RELU => BN layer set\n",
    "        x = self.ct2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        # pass the output from previous layer through our last set\n",
    "        # of CONVT => RELU => BN layers\n",
    "        x = self.ct3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        # pass the output from previous layer through CONVT2D => TANH\n",
    "        # layers to get our output\n",
    "        x = self.ct4(x)\n",
    "        output = self.tanh(x)\n",
    "        # return the output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2eb0d7",
   "metadata": {},
   "source": [
    "##### Below is the Discriminator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216b2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, depth, alpha=0.2):\n",
    "\t\tsuper(Discriminator, self).__init__()\n",
    "\t\t# first set of CONV => RELU layers\n",
    "\t\tself.conv1 = Conv2d(in_channels=depth, out_channels=32,\n",
    "\t\t\t\tkernel_size=4, stride=2, padding=1)\n",
    "\t\tself.leakyRelu1 = LeakyReLU(alpha, inplace=True)\n",
    "\t\t# second set of CONV => RELU layers\n",
    "\t\tself.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=4,\n",
    "\t\t\t\tstride=2, padding=1)\n",
    "\t\tself.leakyRelu2 = LeakyReLU(alpha, inplace=True)\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tself.fc1 = Linear(in_features=3136, out_features=512)\n",
    "\t\tself.leakyRelu3 = LeakyReLU(alpha, inplace=True)\n",
    "\t\t# sigmoid layer outputting a single value\n",
    "\t\tself.fc2 = Linear(in_features=512, out_features=1)\n",
    "\t\tself.sigmoid = Sigmoid()\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# pass the input through first set of CONV => RELU layers\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.leakyRelu1(x)\n",
    "\t\t# pass the output from the previous layer through our second\n",
    "\t\t# set of CONV => RELU layers\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = self.leakyRelu2(x)\n",
    "\t\t# flatten the output from the previous layer and pass it\n",
    "\t\t# through our first (and only) set of FC => RELU layers\n",
    "\t\tx = flatten(x, 1)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.leakyRelu3(x)\n",
    "\t\t# pass the output from the previous layer through our sigmoid\n",
    "\t\t# layer outputting a single value\n",
    "\t\tx = self.fc2(x)\n",
    "\t\toutput = self.sigmoid(x)\n",
    "\t\t# return the output\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f791d",
   "metadata": {},
   "source": [
    "Training The DCGAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
