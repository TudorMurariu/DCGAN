{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd23dc2",
   "metadata": {},
   "source": [
    "# I'll try to use our DCGAN model on a dataset of planes to see if we can make it generate soething that looks close to a plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68753cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LeakyReLU\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Tanh\n",
    "from torch.nn import Sigmoid\n",
    "from torch import flatten\n",
    "from torch import nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, inputDim=100, outputChannels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        # first set of CONVT => RELU => BN\n",
    "        self.ct1 = ConvTranspose2d(in_channels=inputDim,\n",
    "            out_channels=128, kernel_size=4, stride=1, padding=0,\n",
    "            bias=False)\n",
    "        self.relu1 = ReLU()\n",
    "        self.batchNorm1 = BatchNorm2d(128)\n",
    "        # second set of CONVT => RELU => BN\n",
    "        self.ct2 = ConvTranspose2d(in_channels=128, out_channels=64,\n",
    "                    kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.relu2 = ReLU()\n",
    "        self.batchNorm2 = BatchNorm2d(64)\n",
    "        # last set of CONVT => RELU => BN\n",
    "        self.ct3 = ConvTranspose2d(in_channels=64, out_channels=32,\n",
    "                    kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.relu3 = ReLU()\n",
    "        self.batchNorm3 = BatchNorm2d(32)\n",
    "        # apply another upsample and transposed convolution, but\n",
    "        # this time output the TANH activation\n",
    "        self.ct4 = ConvTranspose2d(in_channels=32,\n",
    "            out_channels=outputChannels, kernel_size=4, stride=2,\n",
    "            padding=1, bias=False)\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ct1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batchNorm1(x)\n",
    "\n",
    "        x = self.ct2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batchNorm2(x)\n",
    "\n",
    "        x = self.ct3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.ct4(x)\n",
    "        \n",
    "        output = self.tanh(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, depth, alpha=0.2):\n",
    "\t\tsuper(Discriminator, self).__init__()\n",
    "\t\t\n",
    "\t\tself.conv1 = Conv2d(in_channels=depth, out_channels=32,\n",
    "\t\t\t\tkernel_size=4, stride=2, padding=1)\n",
    "\t\tself.leakyRelu1 = LeakyReLU(alpha, inplace=True)\n",
    "\t\t\n",
    "\t\tself.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=4,\n",
    "\t\t\t\tstride=2, padding=1)\n",
    "\t\tself.leakyRelu2 = LeakyReLU(alpha, inplace=True)\n",
    "\t\t\n",
    "\t\tself.fc1 = Linear(in_features=4096, out_features=512)\n",
    "\t\tself.leakyRelu3 = LeakyReLU(alpha, inplace=True)\n",
    "\t\t\n",
    "\t\tself.fc2 = Linear(in_features=512, out_features=1)\n",
    "\t\tself.sigmoid = Sigmoid()\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.leakyRelu1(x)\n",
    "\t\t\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = self.leakyRelu2(x)\n",
    "\t\t\n",
    "\t\tx = flatten(x, 1)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.leakyRelu3(x)\n",
    "\t\t\n",
    "\t\tx = self.fc2(x)\n",
    "\t\toutput = self.sigmoid(x)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d838b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from sklearn.utils import shuffle\n",
    "from imutils import build_montages\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def weights_init(model):\n",
    "\tclassname = model.__class__.__name__\n",
    "\t\n",
    "\tif classname.find(\"Conv\") != -1:\n",
    "\t\tnn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "\t\n",
    "\telif classname.find(\"BatchNorm\") != -1:\n",
    "\t\tnn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "\t\tnn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f0268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06feb5",
   "metadata": {},
   "source": [
    "##### Let's get setup a Dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3d60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\n",
    "\n",
    "from torchvision import transforms\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())\n",
    "\n",
    "import torch\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "mean = imgs.view(3, -1).mean(dim=1)\n",
    "std = imgs.view(3, -1).std(dim=1)\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "        data_path, train=True, download=False,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean,\n",
    "                std)\n",
    "        ]))\n",
    "transformed_cifar10_test = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    ")\n",
    "\n",
    "label_map = {0: 0}\n",
    "class_names = ['airplane']\n",
    "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10\n",
    "    if label == 0]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in transformed_cifar10_test\n",
    "    if label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9632247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "305ec141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building generator...\n",
      "[INFO] building discriminator...\n"
     ]
    }
   ],
   "source": [
    "stepsPerEpoch = len(val_loader.dataset) // BATCH_SIZE\n",
    "\n",
    "print(\"[INFO] building generator...\")\n",
    "gen = Generator(inputDim=100, outputChannels=3)\n",
    "gen.apply(weights_init)\n",
    "\n",
    "print(\"[INFO] building discriminator...\")\n",
    "disc = Discriminator(depth=3)\n",
    "disc.apply(weights_init)\n",
    "\n",
    "# initialize optimizer for both generator and discriminator\n",
    "genOpt = Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999),\n",
    "\tweight_decay=0.0002 / NUM_EPOCHS)\n",
    "discOpt = Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999),\n",
    "\tweight_decay=0.0002 / NUM_EPOCHS)\n",
    "\n",
    "criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22539888",
   "metadata": {},
   "source": [
    "### Now the Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87d3e931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting training...\n",
      "[INFO] starting epoch 1 of 100...\n",
      "Flattened shape: torch.Size([64, 4096])\n",
      "Flattened shape: torch.Size([64, 3136])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x3136 and 4096x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m labels\u001b[38;5;241m.\u001b[39mfill_(fakeLabel)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# perform a forward pass through discriminator using fake\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# batch data\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdisc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m errorFake \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     35\u001b[0m errorFake\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Tudor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tudor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m flatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlattened shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleakyRelu3(x)\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\Tudor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tudor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tudor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x3136 and 4096x512)"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] starting training...\")\n",
    "benchmarkNoise = torch.randn(256, 100, 1, 1)\n",
    "\n",
    "realLabel = 1\n",
    "fakeLabel = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\tprint(\"[INFO] starting epoch {} of {}...\".format(epoch + 1,\n",
    "\t\tNUM_EPOCHS))\n",
    "\t\n",
    "\tepochLossG = 0\n",
    "\tepochLossD = 0\n",
    "\tfor x in val_loader:\n",
    "\t\tdisc.zero_grad()\n",
    "\t\t\n",
    "\t\timages = x[0]\n",
    "\t\t\n",
    "\t\tbs =  images.size(0)\n",
    "\t\tlabels = torch.full((bs,), realLabel, dtype=torch.float)\n",
    "\t\t\n",
    "\t\toutput = disc(images).view(-1)\n",
    "\t\t# calculate the loss on all-real batch\n",
    "\t\terrorReal = criterion(output, labels)\n",
    "\t\terrorReal.backward()\n",
    "\n",
    "\t\tnoise = torch.randn(bs, 100, 1, 1)\n",
    "\t\t# generate a fake image batch using the generator\n",
    "\t\tfake = gen(noise)\n",
    "\t\tlabels.fill_(fakeLabel)\n",
    "\n",
    "\t\t# perform a forward pass through discriminator using fake\n",
    "\t\t# batch data\n",
    "\t\toutput = disc(fake.detach()).view(-1)\n",
    "\t\terrorFake = criterion(output, labels)\n",
    "\t\terrorFake.backward()\n",
    "\t\terrorD = errorReal + errorFake\n",
    "\t\tdiscOpt.step()\n",
    "\n",
    "\t\tgen.zero_grad()\n",
    "\t\t# update the labels as fake labels are real for the generator\n",
    "\t\t# and perform a forward pass  of fake data batch through the\n",
    "\t\t# discriminator\n",
    "\t\tlabels.fill_(realLabel)\n",
    "\t\toutput = disc(fake).view(-1)\n",
    "\t\t# calculate generator's loss based on output from\n",
    "\t\t# discriminator and calculate gradients for generator\n",
    "\t\terrorG = criterion(output, labels)\n",
    "\t\terrorG.backward()\n",
    "\t\t# update the generator\n",
    "\t\tgenOpt.step()\n",
    "\t\t# add the current iteration loss of discriminator and\n",
    "\t\t# generator\n",
    "\t\tepochLossD += errorD\n",
    "\t\tepochLossG += errorG\n",
    "\n",
    "\tprint(\"[INFO] Generator Loss: {:.4f}, Discriminator Loss: {:.4f}\".format(\n",
    "\t\tepochLossG / stepsPerEpoch, epochLossD / stepsPerEpoch))\n",
    "\t\n",
    "\tif (epoch + 1) % 2 == 0:\n",
    "\t\t# set the generator in evaluation phase, make predictions on\n",
    "\t\t# the benchmark noise, scale it back to the range [0, 255],\n",
    "\t\t# and generate the montage\n",
    "\t\tgen.eval()\n",
    "\t\timages = gen(benchmarkNoise)\n",
    "\t\timages = images.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "\t\timages = ((images * 127.5) + 127.5).astype(\"uint8\")\n",
    "\t\tvis = build_montages(images, (64, 64), (16, 16))[0]\n",
    "\t\tvis = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)\n",
    "\t\t\n",
    "\t\t# save output\n",
    "\t\tp = os.path.join('output', \"epoch_{}.png\".format(\n",
    "\t\t\tstr(epoch + 1).zfill(4)))\n",
    "\t\tcv2.imwrite(p, vis)\n",
    "\t\tgen.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
